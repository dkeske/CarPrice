{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlContext' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-939b0955cc90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Dataframe creation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcarsDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT * FROM cars\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcarsDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcarsDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sqlContext' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Dataframe creation\n",
    "carsDF = sqlContext.sql(\"SELECT * FROM cars\")\n",
    "carsDF.cache()\n",
    "display(carsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def parseReg (x):\n",
    "  if x != 'Nije registrovan': \n",
    "    return 'Registrovan' \n",
    "  else : \n",
    "    return x\n",
    "def parseInt(x):\n",
    "  if x == \"\":\n",
    "    return u'?tof'\n",
    "  else :\n",
    "    return x\n",
    "regUdf = UserDefinedFunction(parseReg, StringType())\n",
    "intUdf = UserDefinedFunction(parseInt, StringType())\n",
    "carsDF = carsDF.select(\"*\", regUdf(\"registration\").alias(\"reg\")).drop(\"registration\")\n",
    "carsDF = carsDF.select(\"*\", intUdf(\"interiorMaterial\").alias('interior')).drop('interiorMaterial')\n",
    "carsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "prepPipeline = Pipeline()\n",
    "prepStages = [\n",
    "  StringIndexer(inputCol=\"fuelType\", outputCol=\"fuelTypeIndex\"),\n",
    "  OneHotEncoder(dropLast=True, inputCol=\"fuelTypeIndex\", outputCol=\"fuelTypeVec\"),\n",
    "  StringIndexer(inputCol=\"chassis\", outputCol=\"chassisIndex\"),\n",
    "  OneHotEncoder(dropLast=True, inputCol=\"chassisIndex\", outputCol=\"chassisVec\"),\n",
    "  StringIndexer(inputCol=\"reg\", outputCol=\"regIndex\"),\n",
    "  OneHotEncoder(dropLast=True, inputCol=\"regIndex\", outputCol=\"regVec\"),\n",
    "  StringIndexer(inputCol=\"userType\", outputCol=\"userTypeIndex\"),\n",
    "  OneHotEncoder(dropLast=True, inputCol=\"userTypeIndex\", outputCol=\"userTypeVec\"),\n",
    "  StringIndexer(inputCol=\"gearBox\", outputCol=\"gearBoxIndex\"),\n",
    "  OneHotEncoder(dropLast=True, inputCol=\"gearBoxIndex\", outputCol=\"gearBoxVec\"),\n",
    "  StringIndexer(inputCol=\"interior\", outputCol=\"interiorIndex\"),\n",
    "  OneHotEncoder(dropLast=True, inputCol=\"interiorIndex\", outputCol=\"interiorVec\")\n",
    "]\n",
    "prepPipeline.setStages(prepStages)\n",
    "carsDF = prepPipeline.fit(carsDF).transform(carsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF.select(\"userTypeVec\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import *\n",
    "# customSchema = StructType([ \\\n",
    "#     StructField('mileage', IntegerType(), True), \\\n",
    "#     StructField('year', IntegerType(), True), \\\n",
    "#     StructField('fuelType', StringType(), True), \\\n",
    "#     StructField('power', IntegerType(), True), \\\n",
    "#     StructField('price', DoubleType(), True), \\\n",
    "#     StructField('engineVolume', DoubleType(), True)])\n",
    "\n",
    "# carsDF = sqlContext.createDataFrame(carsDF, customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print carsDF.dtypes\n",
    "# display(carsDF.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF = carsDF.filter(\"power > 30\")\\\n",
    "               .filter(\"price < 99999999\")\\\n",
    "               .filter(\"engineVolume > 400\")\\\n",
    "               .filter(\"engineVolume < 3500\")\\\n",
    "#                .filter(\"mileage > 3000\")\\\n",
    "#                .filter(\"year < 2015\")\n",
    "display(carsDF.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualisation...\n",
    "display(carsDF.select(\"price\" , \"engineVolume\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "carsDF = carsDF.select(\"mileage\", \"year\", \"engineVolume\", \"power\", \"fuelTypeVec\", \"chassisVec\", \"regVec\", \"userTypeVec\", \"gearBoxVec\", \"interiorVec\", \"price\")\n",
    "\n",
    "vectorizer = VectorAssembler()\n",
    "vectorizer.setInputCols([\"mileage\", \"year\", \"engineVolume\", \"power\", \"fuelTypeVec\", \"chassisVec\", \"regVec\", \"userTypeVec\", \"gearBoxVec\", \"interiorVec\"])\n",
    "vectorizer.setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1800009193L\n",
    "(split20DF, split80DF) = carsDF.randomSplit([20.0, 80.0], seed)\n",
    "\n",
    "# Let's cache these datasets for performance\n",
    "testSetDF = split20DF.cache()\n",
    "trainingSetDF = split80DF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "# Let's initialize our linear regression learner\n",
    "lr = LinearRegression()\n",
    "lr.setSolver(\"l-bfgs\")\n",
    "# We use explain params to dump the parameters we can use\n",
    "# print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we set the parameters for the method\n",
    "lr.setPredictionCol(\"Predicted_price\")\\\n",
    "  .setLabelCol(\"price\")\\\n",
    "  .setMaxIter(100)\\\n",
    "  .setRegParam(0.1)\n",
    "\n",
    "\n",
    "# We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\n",
    "lrPipeline = Pipeline()\n",
    "\n",
    "lrPipeline.setStages([vectorizer, lr])\n",
    "\n",
    "# Let's first train on the entire dataset to see what we get\n",
    "# lrModel = lrPipeline.fit(trainingSetDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create an RMSE evaluator using the label and predicted columns\n",
    "regEval = RegressionEvaluator(predictionCol=\"Predicted_price\", labelCol=\"price\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# We can reuse the RegressionEvaluator, regEval, to judge the model based on the best Root Mean Squared Error\n",
    "# Let's create our CrossValidator with 3 fold cross validation\n",
    "crossval = CrossValidator(estimator=lrPipeline, evaluator=regEval, numFolds=3)\n",
    "\n",
    "# Let's tune over our regularization parameter from 0.01 to 0.10\n",
    "regParam = [0.09, 0.1]\n",
    "\n",
    "# We'll create a paramter grid using the ParamGridBuilder, and add the grid to the CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, regParam)\n",
    "             .build())\n",
    "crossval.setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "# Now let's find and return the best model\n",
    "lrModel = crossval.fit(trainingSetDF).bestModel\n",
    "print lrModel.stages[-1]._java_obj.parent().getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the function:\n",
    "intercept = lrModel.stages[1].intercept\n",
    "print intercept\n",
    "\n",
    "# The coefficents (i.e., weights) are as follows:\n",
    "weights = lrModel.stages[1].coefficients\n",
    "print weights\n",
    "\n",
    "# Create a list of the column names (without PE)\n",
    "featuresNoLabel = [col for col in carsDF.columns if col != \"price\" and not col.endswith(\"Index\")]\n",
    "print featuresNoLabel\n",
    "\n",
    "# Merge the weights and labels\n",
    "coefficents = zip(weights, featuresNoLabel)\n",
    "for coef in coefficents:\n",
    "  print coef\n",
    "\n",
    "# Now let's sort the coefficients from greatest absolute weight most to the least absolute weight\n",
    "coefficents.sort(key=lambda tup: abs(tup[0]), reverse=True)\n",
    "\n",
    "equation = \"y = {intercept}\".format(intercept=intercept)\n",
    "variables = []\n",
    "for x in coefficents:\n",
    "    weight = abs(x[0])\n",
    "    name = x[1]\n",
    "    symbol = \"+\" if (x[0] > 0) else \"-\"\n",
    "    equation += (\" {} ({} * {})\".format(symbol, weight, name))\n",
    "\n",
    "# Finally here is our equation\n",
    "print(\"Linear Regression Equation: \" + equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our LR model to the test data and predict power output\n",
    "predictionsAndLabelsDF = lrModel.transform(testSetDF).select('price', \"Predicted_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Create a DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "dt.setLabelCol(\"price\")\\\n",
    "  .setPredictionCol(\"Predicted_price\")\\\n",
    "  .setFeaturesCol(\"features\")\\\n",
    "  .setMaxBins(100)\n",
    "\n",
    "# Create a Pipeline\n",
    "dtPipeline = Pipeline()\n",
    "\n",
    "\n",
    "# Set the stages of the Pipeline\n",
    "dtPipeline.setStages([vectorizer, dt])\n",
    "\n",
    "# dtModel = dtPipeline.fit(trainingSetDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval.setEstimator(dtPipeline)\n",
    "\n",
    "# Let's tune over our dt.maxDepth parameter on the values 2 and 3, create a paramter grid using the ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [2,3,4])\n",
    "             .build())\n",
    "\n",
    "# Add the grid to the CrossValidator\n",
    "crossval.setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "# Now let's find and return the best model\n",
    "dtModel = crossval.fit(trainingSetDF).bestModel\n",
    "\n",
    "print dtModel.stages[-1]._java_obj.parent().getMaxDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "\n",
    "rf.setLabelCol(\"price\")\\\n",
    "  .setPredictionCol(\"Predicted_price\")\\\n",
    "  .setFeaturesCol(\"features\")\\\n",
    "  .setSeed(100088121L)\\\n",
    "  .setMaxDepth(8)\\\n",
    "  .setNumTrees(20)\n",
    "\n",
    "# Create a Pipeline\n",
    "rfPipeline = Pipeline()\n",
    "\n",
    "# Set the stages of the Pipeline\n",
    "rfPipeline.setStages([vectorizer, rf])\n",
    "\n",
    "# rfModel = rfPipeline.fit(trainingSetDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval.setEstimator(rfPipeline)\n",
    "\n",
    "# Let's tune over our rf.maxBins parameter on the values 50 and 100, create a parameter grid using the ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [20, 30, 40])\n",
    "             .addGrid(rf.maxBins, [50, 70, 100])\n",
    "             .build())\n",
    "\n",
    "# Add the grid to the CrossValidator\n",
    "crossval.setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "# Now let's find and return the best model\n",
    "rfModel = crossval.fit(trainingSetDF).bestModel\n",
    "print rfModel.stages[-1]._java_obj.parent().getNumTrees()\n",
    "print rfModel.stages[-1]._java_obj.parent().getMaxBins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compute an evaluation metric for our test dataset\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create an RMSE evaluator using the label and predicted columns\n",
    "regEval = RegressionEvaluator(predictionCol=\"Predicted_price\", labelCol=\"price\", metricName=\"rmse\")\n",
    "\n",
    "# Run the evaluator on the DataFrame\n",
    "rmse = regEval.evaluate(predictionsAndLabelsDF)\n",
    "\n",
    "# print(\"Root Mean Squared Error: %.2f\" % rmse)\n",
    "# Now let's compute another evaluation metric for our test dataset\n",
    "r2 = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n",
    "\n",
    "# print(\"r2: {0:.2f}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use dtModel to compute an evaluation metric for our test dataset: testSetDF\n",
    "predictionsAndLabelsDF = dtModel.transform(testSetDF)\n",
    "\n",
    "# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\n",
    "rmseDT = regEval.evaluate(predictionsAndLabelsDF)\n",
    "\n",
    "# Now let's compute the r2 evaluation metric for our test dataset\n",
    "r2DT = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName:\"r2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use rfModel to compute an evaluation metric for our test dataset: testSetDF\n",
    "predictionsAndLabelsDF = rfModel.transform(testSetDF)\n",
    "\n",
    "# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\n",
    "rmseRF = regEval.evaluate(predictionsAndLabelsDF)\n",
    "\n",
    "# Now let's compute the r2 evaluation metric for our test dataset\n",
    "r2RF = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName:\"r2\"})\n",
    "\n",
    "print(\"LR Root Mean Squared Error: {0:.2f}\".format(rmse))\n",
    "print(\"DT Root Mean Squared Error: {0:.2f}\".format(rmseDT))\n",
    "print(\"RF Root Mean Squared Error: {0:.2f}\".format(rmseRF))\n",
    "print(\"LR r2: {0:.2f}\".format(r2))\n",
    "print(\"DT r2: {0:.2f}\".format(r2DT))\n",
    "print(\"RF r2: {0:.2f}\".format(r2RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"DROP TABLE IF EXISTS Power_Plant_RMSE_Evaluation\")\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/Power_Plant_RMSE_Evaluation\", True)\n",
    "\n",
    "# Next we calculate the residual error and divide it by the RMSE\n",
    "predictionsAndLabelsDF.selectExpr(\"price\", \"Predicted_price\", \"price - Predicted_price Residual_Error\", \"(price - Predicted_price) / {} Within_RSME\".format(rmseRF)).registerTempTable(\"Cars_RF_RMSE_Evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1\n",
    "            when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3\n",
    "       end RSME_Multiple, COUNT(*) AS count\n",
    "FROM Cars_RF_RMSE_Evaluation\n",
    "GROUP BY case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1  when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3 end"
   ]
  }
 ],
 "metadata": {
  "name": "cars_ml",
  "notebookId": 9.68739455020844E14
 },
 "nbformat": 4,
 "nbformat_minor": 0
}